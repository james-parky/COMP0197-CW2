# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UmGxjnq2zYsNIMIYg0Wn0g3opzjYOwpJ
"""

"Module for downloading the new cat and dog datasets through gdown."
import os
import shutil
import gdown  # pylint: disable=import-error


TRAINING_DATASET_FILE_ID = "14B78jkv9L-6UJheRbHgCDyysq5v4GF3Q"
TRAINING_DATASET_PATH_ZIP = "./training_dataset.zip"
CAT_DATASET_PATH = "./training_dataset"


def download_file_from_google_drive(file_id, output_path):
    """
    Download a file from Google Drive given its file ID and save it to the specified
    output path.

    Args:
        file_id (str): The ID of the file to download from Google Drive.
        output_path (str): The path where the downloaded file will be saved.
    """
    if not os.path.exists(output_path):
        url = f"https://drive.google.com/uc?id={file_id}"
        gdown.download(url, output_path, quiet=False)
        print(f"Downloaded file: {output_path}")
    else:
        print(f"File already exists: {output_path}")


# def download_file_from_google_drive(file_id, output_path):
#     """
#     Download a file from Google Drive given its file ID and save it to the specified output path.
#
#     Args:
#         file_id (str): The ID of the file to download from Google Drive.
#         output_path (str): The path where the downloaded file will be saved.
#     """
#     url = f"https://drive.google.com/uc?id={file_id}&export=download"
#     session = requests.Session()
#     response = session.get(url, stream=True)
#
#     # Check if the response is an HTML page with a virus scan warning
#     if response.headers.get('Content-Type') == 'text/html':
#         # Extract the download form data from the HTML page
#         download_form_data = {}
#         for input_tag in re.findall(r'<input\s.*?>', response.text):
#             name = re.search(r'name="(.*?)"', input_tag).group(1)
#             value = re.search(r'value="(.*?)"', input_tag).group(1)
#             download_form_data[name] = value
#
#         # Submit the download form to confirm the download
#         download_url = 'https://drive.usercontent.google.com/download'
#         response = session.post(download_url, data=download_form_data, stream=True)
#
#     # Save the file to disk
#     with open(output_path, 'wb') as file:
#         shutil.copyfileobj(response.raw, file)
#
#     print(f"Downloaded file: {output_path}")
#     del response


if __name__ == "__main__":
    # Download training dataset
    os.makedirs(os.path.dirname(TRAINING_DATASET_PATH_ZIP), exist_ok=True)
    download_file_from_google_drive(
        TRAINING_DATASET_FILE_ID, TRAINING_DATASET_PATH_ZIP
    )

    # Extract the downloaded datasets if not already extracted
    if not os.path.exists(os.path.join(CAT_DATASET_PATH, "images")):
        shutil.unpack_archive(TRAINING_DATASET_PATH_ZIP, CAT_DATASET_PATH)
        print("Extracted training dataset.")
    else:
        print("Training dataset already extracted.")

"""
Module to create the new dataset.
"""
import os
import random
import shutil
from typing import Tuple
from torch.utils.data import Dataset, DataLoader  # pylint: disable=import-error
from PIL import Image
from torchvision import transforms  # pylint: disable=import-error
import random
# Specify the source folders and destination folder
SOURCE_FOLDER_1 = "./cats_filtered"
SOURCE_FOLDER_2 = "./dogs_filtered"
DESTINATION_FOLDER = "./training_dataset/training_dataset"


class ImageDataset(Dataset):
    """
    A custom PyTorch dataset for images.

    Args:
        root_dir (str): Root directory path containing the dataset.
        transform (callable, optional): Optional transform to be applied on a sample.
    """

    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.samples = self._make_dataset()

    def __len__(self):
        """
        Return the number of samples in the dataset.

        Returns:
            int: Number of samples in the dataset.
        """
        return len(self.samples)

    def __getitem__(self, idx):
        """
        Get a sample from the dataset at the given index.

        Args:
            idx (int): Index of the sample to retrieve.

        Returns:
            tuple: A tuple containing the image and a placeholder target (e.g., -1).
        """
        img_path = self.samples[idx]
        img = Image.open(img_path).convert("RGB")
        if self.transform:
            img = self.transform(img)
        return img, -1  # Return a placeholder target (e.g., -1)

    def _make_dataset(self):
        """
        Create the dataset by collecting image paths.

        Returns:
            list: List of image paths.
        """
        samples = []
        for img_name in os.listdir(self.root_dir):
            if img_name.endswith((".jpg", ".jpeg", ".png")):
                img_path = os.path.join(self.root_dir, img_name)
                samples.append(img_path)
        return samples

    # def download_file_from_google_drive(file_id, output_path):
    # """
    # Download a file from Google Drive given its file ID and save it to the specified output path.
    #
    # Args:
    #     file_id (str): The ID of the file to download from Google Drive.
    #     output_path (str): The path where the downloaded file will be saved.
    # """
    # if not os.path.exists(output_path):
    #     url = f"https://drive.google.com/uc?id={file_id}"
    #     gdown.download(url, output_path, quiet=False)
    #     print(f"Downloaded file: {output_path}")
    # else:
    #     print(f"File already exists: {output_path}")
    #
    # def download_file_from_google_drive(file_id, output_path):
    #     """
    #     Download a file from Google Drive given its file ID and save it to the specified
    #     output path.
    #
    #     Args:
    #         file_id (str): The ID of the file to download from Google Drive.
    #         output_path (str): The path where the downloaded file will be saved.
    #     """
    #     url = f"https://drive.google.com/uc?id={file_id}&export=download"
    #     session = requests.Session()
    #     response = session.get(url, stream=True)
    #
    #     # Check if the response is an HTML page with a virus scan warning
    #     if response.headers.get('Content-Type') == 'text/html':
    #         # Extract the download form data from the HTML page
    #         download_form_data = {}
    #         for input_tag in re.findall(r'<input\s.*?>', response.text):
    #             name = re.search(r'name="(.*?)"', input_tag).group(1)
    #             value = re.search(r'value="(.*?)"', input_tag).group(1)
    #             download_form_data[name] = value
    #
    #         # Submit the download form to confirm the download
    #         download_url = 'https://drive.usercontent.google.com/download'
    #         response = session.post(download_url, data=download_form_data, stream=True)
    #
    #     # Save the file to disk
    #     with open(output_path, 'wb') as file:
    #         shutil.copyfileobj(response.raw, file)
    #
    #     print(f"Downloaded file: {output_path}")
    #     del response


def subsample_images(source_folder, num_images=200):
    """
    Randomly sub-sample images from a source folder containing multiple sub-folders,
    each containing multiple images. The sub-sampled images are then moved to a
    separate destination folder.

    Args:
        source_folder (str): Path to the source folder containing sub-folders with
                             images.
        DESTINATION_FOLDER (str): Path to the destination folder where sub-sampled
                                  images will be moved.
        num_images (int): Number of images to sub-sample from each sub-folder
                          (default is 200).
    """
    os.makedirs(DESTINATION_FOLDER, exist_ok=True)

    for subfolder in os.listdir(source_folder):
        subfolder_path = os.path.join(source_folder, subfolder)

        if os.path.isdir(subfolder_path):
            image_files = [
                f
                for f in os.listdir(subfolder_path)
                if f.endswith((".jpg", ".jpeg", ".png"))
            ]

            num_subsample = min(len(image_files), num_images)

            subsampled_images = random.sample(image_files, num_subsample)

            for image in subsampled_images:
                source_path = os.path.join(subfolder_path, image)
                destination_path = os.path.join(DESTINATION_FOLDER, image)
                shutil.copy(source_path, destination_path)


def count_files(folder_path):
    """
    Count the number of files within a specified folder.

    Args:
        folder_path (str): Path to the folder.

    Returns:
        int: Number of files in the folder.

    Raises:
        ValueError: If the specified path is not a valid directory.
    """
    if not os.path.isdir(folder_path):
        raise ValueError(f"{folder_path} is not a valid directory.")

    file_list = os.listdir(folder_path)
    file_count = len(file_list)

    return file_count


class UnsupervisedLoader:
    def __init__(
        self, img_size: Tuple[int, int] = (224, 224), batch_size: int = 32
    ):
        self.img_size = img_size
        self.batch_size = batch_size

    def build(self, data_augmentation=None):
        # Sub-sample images from the source folders
        # subsample_images(SOURCE_FOLDER_1)
        # subsample_images(SOURCE_FOLDER_2)

        # Define the data transformation

        base_transforms = [
            transforms.Resize(self.img_size),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]
            ),
            transforms.RandomGrayscale(p=0.5),
            transforms.RandomHorizontalFlip(p=0.5)


        ]

        transform_composition = transforms.Compose(
            data_augmentation#+ base_transforms
            if data_augmentation
            else base_transforms
        )

        # Create the dataset and dataloader
        dataset = ImageDataset(
            DESTINATION_FOLDER, transform=transform_composition
        )
        dataloader = DataLoader(
            dataset, batch_size=self.batch_size, shuffle=True, num_workers=0
        )
        return dataloader

!pip install lightly

"""
----------------
Module Imports
----------------
"""
import os
import ssl
import torch as tc
import torchvision


class OxfordData:
    """
    OxfordData class for downloading the Oxford-IIIT Pet dataset and storing it for use.
    """

    def __init__(self, root="../data/oxford_pets", transform=None):
        """
        Initialises OxfordData object.

        :param root: String representing the root data folder, by default this
                     is '/data' in top level of '/src' (when called from the 'supervised' folder).
        :param transform: torchvision.transforms object defining transforms to
                          be performed on data upon download, by default this is 'None'.
        :return: 'None'; nothing is returned.
        """
        # Prevent certificate verification error when downloading Torchvision datasets
        ssl._create_default_https_context = ssl._create_unverified_context

        self.root = root.replace("/", os.path.sep)
        self.trainroot = os.path.join(self.root, "train")
        self.testroot = os.path.join(self.root, "test")
        if transform:
            self.transform = transform
        else:
            # Default transformations
            self.transform = torchvision.transforms.Compose(
                [
                    torchvision.transforms.Resize((224, 224)),
                    torchvision.transforms.ToTensor(),
                    torchvision.transforms.Normalize(
                        (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)
                    ),
                ]
            )
        self.ingest_data()

    def ingest_data(self):
        """
        Downloads the Oxford-IIIT Pet dataset into the object's pre-defined root folder.

        :return: 'None'; nothing is returned.
        """
        self.train = torchvision.datasets.OxfordIIITPet(
            root=self.trainroot,
            split="trainval",
            transform=self.transform,
            download=True,
        )
        self.test = torchvision.datasets.OxfordIIITPet(
            root=self.testroot,
            split="test",
            transform=self.transform,
            download=True,
        )

    def get_train(self):
        """
        Returns the training data.

        :return: A TorchVision dataset stored in the 'self.train' instance variable or 'None'.
        """
        if self.train:
            return self.train
        raise ValueError("OxfordData: No training data to return.")

    def get_test(self):
        """
        Returns the training data.

        :return: A TorchVision dataset stored in the 'self.train' instance variable or 'None'.
        """
        if self.test:
            return self.test
        raise ValueError("OxfordData: No test data to return.")

    def get_subset(self, subset, size):
        """
        Returns a subset of the specified dataset of the specified size.

        :param set: A string representing the choice of dataset,
                    "test" being for self.test and self.train being the default.
        :param size: An integer representing the size of the desired subset.
        :return: A TorchVision dataset that is a subset of either 'self.test'
                 or 'self.train' instance variables.
        """
        if subset == "test":
            data = self.test
        else:
            data = self.train
        indices = list(range(size))

        return tc.utils.data.Subset(data, indices)


# NOTE: Refactor into generic data class that can use different roots and torchvision.dataset.

# Note: The model and training settings do not follow the reference settings
# from the paper. The settings are chosen such that the example can easily be
# run on a small dataset with a single GPU.
import torch
import torchvision
from torch import nn
from lightly.loss import NTXentLoss
from lightly.models.modules import SimCLRProjectionHead
from lightly.transforms.simclr_transform import SimCLRTransform
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)
# from unsupervised_loader import UnsupervisedLoader


class SimCLR(nn.Module):
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        self.projection_head = SimCLRProjectionHead(512, 512, 128)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),  # Output: [32, 64, 2, 2]
            nn.ReLU(),
            nn.ConvTranspose2d(64, 64, 3, stride=2, padding=1, output_padding=1),  # Output: [32, 64, 4, 4]
            nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),  # Output: [32, 32, 8, 8]
            nn.ReLU(),
            nn.ConvTranspose2d(32, 32, 3, stride=2, padding=1, output_padding=1),  # Output: [32, 32, 16, 16]
            nn.ReLU(),
            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=1),  # Output: [32, 16, 32, 32]
            nn.ReLU(),
            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=1)
        )

    def forward(self, x, return_embeddings=False):
        x = self.backbone(x).flatten(start_dim=1)
        z = self.projection_head(x)
        if return_embeddings:
          return z
        z = z.view(-1,128,1,1)
        z = self.decoder(z)
        return z

class UnsupervisedLoader:
    def __init__(
        self, img_size = (224, 224), batch_size: int = 32
    ):
        self.img_size = img_size
        self.batch_size = batch_size

    def build(self, data_augmentation=None):
        """
        Apply the optional data augmentation transforms if they exist and return the
        dataloader.
        """
        # Sub-sample images from the source folders
        # subsample_images(SOURCE_FOLDER_1)
        # subsample_images(SOURCE_FOLDER_2)

        # Define the data transformation

        base_transforms = [
            transforms.Resize(self.img_size),
            transforms.ToTensor(),

            # transforms.RandomGrayscale(p=0.5),
            # transforms.RandomHorizontalFlip(p=0.5),
        ]
        print(data_augmentation)
        transform_composition = transforms.Compose(
                data_augmentation
        )
        print(transform_composition)

        # Create the dataset and dataloader
        dataset = ImageDataset(
            DESTINATION_FOLDER, transform=transform_composition
        )
        dataloader = DataLoader(
            dataset, batch_size=self.batch_size, shuffle=True, num_workers=0
        )
        return dataloader

device = "cuda" if torch.cuda.is_available() else "cpu"
def train_simclr(model, num_epochs, t):

    model.to(device)
    # transform = SimCLRTransform(input_size=32, gaussian_blur=0.0)
    #
    #
    # dataset = torchvision.datasets.CIFAR10(
    #     "datasets/cifar10", download=True, transform=transform
    # )


# or create a dataset from a folder containing images or videos:
# dataset = LightlyDataset("path/to/folder", transform=transform)

    dataloader = UnsupervisedLoader(img_size=(64,64),batch_size=32).build(data_augmentation=t+[SimCLRTransform(input_size=32, gaussian_blur=0.0)])


    criterion = NTXentLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.06)

    # # Prepare test dataset
    test_transform = SimCLRTransform(input_size=32, gaussian_blur=0.0)
    # test_dataset = torchvision.datasets.CIFAR10(
    #     "datasets/cifar10", download=True, transform=test_transform, train=False
    # )
    # test_dataloader = torch.utils.data.DataLoader(
    #     test_dataset, batch_size=32, shuffle=False, num_workers=8
    # )

    print("Starting Training")
    for epoch in range(num_epochs):
        total_loss = 0
        for i, batch in enumerate(dataloader):
            x0, x1 = batch[0]
            x0 = x0.to(device)
            x1 = x1.to(device)


            z0 = model(x0, return_embeddings=True)
            z1 = model(x1, return_embeddings=True)


            loss = criterion(z0, z1)
            total_loss += loss.detach()


            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            print(f"[Epoch {epoch + 1}, Batch {i + 1}]: {loss.item()}")

        avg_loss = total_loss / len(dataloader)
        print(f"epoch: {epoch + 1}, loss: {avg_loss:.5f}")
            # Evaluate the model on the test dataset
        # model.eval()
        # y_true = []
        # y_pred = []
        # with torch.no_grad():
        #     for batch in dataloader:
        #         x0, x1 = batch[0]
        #         x0 = x0.to(device)
        #         x1 = x1.to(device)

        #         z0 = model(x0)
        #         z1 = model(x1)

        #         # Compute cosine similarity between z0 and z1
        #         similarity = torch.cosine_similarity(z0, z1, dim=1)

        #         # Threshold the similarity scores to obtain predicted labels
        #         y_pred.extend((similarity > 0.5).cpu().numpy())

        #         # Collect true labels
        #         y_true.extend(batch[1].numpy())


        # # Compute evaluation metrics
        # accuracy = accuracy_score(y_true, y_pred)
        # precision = precision_score(y_true, y_pred, average='macro')
        # recall = recall_score(y_true, y_pred, average='macro')
        # f1 = f1_score(y_true, y_pred, average='macro')


        # # Print the evaluation metrics
        # print(f"Accuracy: {accuracy:.4f}")
        # print(f"Precision: {precision:.4f}")
        # print(f"Recall: {recall:.4f}")
        # print(f"F1 Score: {f1:.4f}")

        model.train()


    # Reporting section
    print("Training completed.")








from torch.utils.data import random_split
def finetune_eval():
    transform = torchvision.transforms.Compose([
        torchvision.transforms.Resize((64, 64)),
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])

    target_transform = transforms.Compose([
        torchvision.transforms.Resize((64, 64)),
        torchvision.transforms.ToTensor(),
    ])

    # Oxford IIIT Pets Segmentation dataset loaded via torchvision.
    pets_path_train = os.path.join("final", 'OxfordPets', 'train')
    pets_path_test = os.path.join("final", 'OxfordPets', 'test')
    pets_train_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_train, split="trainval", target_types="segmentation", download=True,transform=transform,target_transform=target_transform)
    pets_test_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_test, split="test", target_types="segmentation", download=True,transform=transform,target_transform=target_transform)
    print(type(pets_train_orig))
    finetune_set, eval_set = random_split(pets_train_orig,[int(0.5*len(pets_train_orig))]*2)
    finetune_dataloader = DataLoader(finetune_set, batch_size=32, shuffle=True)
    eval_dataloader= DataLoader(eval_set, batch_size=32, shuffle=True)

    def label_segmentations(trimap):

        x=(trimap * 255.0 - 1) / 2
        # Change all 0.5 values to 1 (background and border become the same class)
        x[x == 0.5] = 1

        return x


    num_epochs = 1
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(simclr_model.parameters(), lr=1e-4)

    for epoch in range(num_epochs):
        simclr_model.train()
        train_loss = 0.0
        for i, (images, masks) in enumerate(finetune_dataloader):
            images = images.to(device)
            masks = masks.to(device)
            outputs = simclr_model(images)
            masks = label_segmentations(masks)
            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            train_loss += loss.item()
            print(f"[Epoch {epoch+1}/{num_epochs} Batch {i + 1}/{len(finetune_set)//32}]")

        train_loss /= len(finetune_dataloader)
        print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}")

    # Save the fine-tuned model
    # torch.save(simclr_model.state_dict(), 'finetuned_model.pth')
    import numpy as np
    simclr_model.eval()
    # testing
    for epoch in range(1):
        simclr_model.eval()
        ious=[]
        for i, (images, masks) in enumerate(eval_dataloader):
            images = images.to(device)
            masks = masks.to(device)

            outputs = simclr_model(images)
            masks = label_segmentations(masks)
            inter = torch.logical_and(masks, outputs)
            uni = torch.logical_or(masks, outputs)

            ious.append(torch.count_nonzero(inter)/torch.count_nonzero(uni))

            loss = criterion(outputs, masks)
            loss.backward()
            optimizer.zero_grad()
            print(f"[Epoch {epoch+1}/{1} Batch {i + 1}/{len(eval_set)//32}]")


        print(f"Average intersction of union accuracy: {torch.mean(torch.as_tensor(ious))}")

ts =[[
     transforms.RandomGrayscale(p=0.5),
],
     [
            transforms.RandomHorizontalFlip(p=0.5),],
             [
            transforms.RandomGrayscale(p=0.5),
            transforms.RandomHorizontalFlip(p=0.5)],]

for t in ts:
    print(t)
    simclr_model =  SimCLR(
                    nn.Sequential(
                        *list(torchvision.models.resnet18().children())[:-1]
                    )
                )
    simclr_model.to(device)
    train_simclr(simclr_model, num_epochs=1,t=t)
    finetune_eval()


# def init_loader(data, size=20, work=8):
#     """
#     Initialises DataLoaders for training and testing purposes.
#     :param size: The desired batch size to use, by default this is set as 20.
#     :param work: The number of workers to use, by default this is set to be 8.
#     """
#     trainloader = torch.utils.data.DataLoader(
#         data.get_train(),
#         batch_size=size,
#         shuffle=True,
#         num_workers=work,
#     )
#     testloader = torch.utils.data.DataLoader(
#         data.get_test(),
#         batch_size=size,
#         shuffle=True,
#         num_workers=work,
#     )
#     return trainloader,testloader



# # data = OxfordData()
# # trainloader,testloader= init_loader(data)
# criterion = torch.nn.CrossEntropyLoss()
# optimiser = torch.optim.Adam(simclr_model.parameters(), lr=0.001, weight_decay=0.01
#         )
# for epoch in range(10):
#     print(f"\n[EPOCH {epoch+1}]")
#     for i,d in enumerate(trainloader):
#         print(i)
#                 # print(f"Batch: {i}")
#         inputs, labels = d
#         inputs=inputs.to(device)
#         labels=labels.to(device)

#         optimiser.zero_grad()
#         outputs = simclr_model(inputs)
#         loss = criterion(outputs, labels.to(dtype=torch.long))
#         loss.backward()
#         optimiser.step()
#         # test()
# print("\n\nTraining complete")

# total = correct = 0
# print("\n[TEST]")
# for _, data in enumerate(testloader):
#     # print(f"Batch: {i}")
#     inputs, labels = data
#     inputs, labels = inputs.to(device), labels.to(device)

#     output = simclr_model(inputs)
#     _, predictions = torch.max(output, 1)
#     # print("INPUT: ", inputs[0])
#     # print("LABEL: ", labels[0])
#     # print("PREDICTION: ", predictions[0])
#     for i, prediction in enumerate(predictions):
#         total += 1
#         if prediction == labels[i]:
#             correct += 1
# accuracy = correct / total
# print("\nAccuracy Results:")
# print(f"Score: {correct} / {total}")
# print(f"Decimal: {accuracy}")


# resnet = torchvision.models.resnet18()
# backbone = nn.Sequential(*list(resnet.children())[:-1])
# model = SimCLR(backbone)
#
#

import torch
import torch.nn as nn
from transformers import ViTMAEForPreTraining

class SegmentationHead(nn.Module):
    def __init__(self, hidden_size, num_classes):
        super().__init__()
        self.conv = nn.Conv2d(hidden_size, num_classes, kernel_size=1)
        self.upsample = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=False)

    def forward(self, x):
        x = self.conv(x)
        x = self.upsample(x)
        return x

class SegmentationModel(nn.Module):
    def __init__(self, pretrained_model_path, num_classes):
        super().__init__()
        self.encoder = ViTMAEForPreTraining(pretrained_model_path).vit #ViTMAEForPreTraining.from_pretrained(pretrained_model_path).vit
        hidden_size = self.encoder.config.hidden_size
        self.segmentation_head = SegmentationHead(hidden_size, num_classes)

    def forward(self, pixel_values):
        outputs = self.encoder(pixel_values, return_dict=True)
        last_hidden_state = outputs.last_hidden_state
        batch_size, sequence_length, hidden_size = last_hidden_state.shape
        height = width = int(sequence_length ** 0.5)
        last_hidden_state = last_hidden_state.permute(0, 2, 1).reshape(batch_size, hidden_size, height, width)
        segmentation_outputs = self.segmentation_head(last_hidden_state)
        return segmentation_outputs

!pip install transformers

""" Pretraining ViTMAE model """

import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision.datasets import CIFAR10
from torchvision.transforms import transforms
from transformers import (
    ViTMAEForPreTraining,
    ViTMAEConfig,
)  # pylint: disable=import-error


# Define the ViTMAE configuration
config = ViTMAEConfig(
    image_size=64,
    patch_size=4,
    num_channels=3,
    encoder_layers=6,
    decoder_layers=4,
    encoder_num_heads=8,
    decoder_num_heads=8,
    encoder_hidden_size=64,
    decoder_hidden_size=64,
    mlp_ratio=4,
    norm_pix_loss=True,
)

# Create the ViTMAEForPreTraining model
model = ViTMAEForPreTraining(config)

# Define the data transforms
pretraining_transforms = transforms.Compose(
    [
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ]
)

# # Download and create the pretraining dataset
# dataset = CIFAR10(root="./data", train=True, download=True, transform=pretraining_transforms)
# pretraining_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)
#
pretraining_dataloader = UnsupervisedLoader(
    img_size=(64, 64), batch_size=32
).build(data_augmentation=[pretraining_transforms])
# Set up the optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# Pretraining loop
NUM_EPOCHS = 5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

total_steps = len(pretraining_dataloader) * NUM_EPOCHS
CURRENT_STEP = 0

for epoch in range(NUM_EPOCHS):
    model.train()
    for batch_idx, batch in enumerate(pretraining_dataloader):
        images, _ = batch  # Ignore the labels during pretraining
        images = images.to(device)
        outputs = model(pixel_values=images)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        CURRENT_STEP += 1
        if (batch_idx + 1) % 1 == 0:
            print(
                f"Epoch [{epoch+1}/{NUM_EPOCHS}] \
                  Batch [{batch_idx+1}/{len(pretraining_dataloader)}] Loss: {loss.item():.4f}"
            )
        if CURRENT_STEP % 1000 == 0:
            progress = (CURRENT_STEP / total_steps) * 100
            print(f"Progress: {progress:.2f}%")

    print(f"Epoch [{epoch+1}/{NUM_EPOCHS}] Loss: {loss.item():.4f}")

# Save the pretrained model
PRETRAINED_MODEL_PATH = "./pretrained_model.pth"
torch.save(model.state_dict(), PRETRAINED_MODEL_PATH)
print("Pretraining completed. Model saved.")


# ================



# Define the ViTMAE configuration


class SegmentationHead(nn.Module):
    def __init__(self, hidden_size):
        super().__init__()
        # (20, 768, 8, 8)
        self.conv = nn.Conv2d(hidden_size, 1, kernel_size=1)
        # (20, 1, 8, 8)
        self.upsample = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=False)
        # (20, 1, 64, 64)

    def forward(self, x):
        x = self.conv(x)
        x = self.upsample(x)
        return x

class SegmentationModel(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.encoder = model.vit
        # Default hidden_size = 768
        hidden_size = self.encoder.config.hidden_size
        self.segmentation_head = SegmentationHead(hidden_size)

    def forward(self, pixel_values):
        outputs_encoder = self.encoder(pixel_values, return_dict=True)

        # last_hidden_state shape is (batch_size, sequence_length, hidden_size)
        # sequence_length is the number of patches

        last_hidden_state = outputs_encoder.last_hidden_state
        last_hidden_state = last_hidden_state[:, 1:] # remove the [CLS] token

        #(batch_size,seq_length = 64, hidden_size = 768)\

        batch_size, sequence_length, hidden_size = last_hidden_state.shape
        # (20, 768, 8, 8)
        height = width = int(sequence_length ** 0.5)


        last_hidden_state = last_hidden_state.permute(0, 2, 1)
        last_hidden_state2 = last_hidden_state.reshape(batch_size, hidden_size, height, width)
        segmentation_outputs = self.segmentation_head(last_hidden_state2)
        return segmentation_outputs

# Create the ViTMAEForPreTraining model
# model = ViTMAEForPreTraining(config)

# model.load_state_dict(torch.load('pretrained_model.pth'))
# model.to(device)
segmentationModel = SegmentationModel(model).to(device)

transform = torchvision.transforms.Compose([
     torchvision.transforms.Resize((64, 64)),
     torchvision.transforms.ToTensor(),
     torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
 ])

target_transform = transforms.Compose([
     torchvision.transforms.Resize((64, 64)),
     torchvision.transforms.ToTensor(),
 ])


# Convert a float trimap ({1, 2, 3} / 255.0) into a float tensor with
# pixel values in the range 0.0 to 1.0 so that the border pixels
# can be properly displayed.
def label_segmentations(trimap):

    x=(trimap * 255.0 - 1) / 2
    # Change all 0.5 values to 1 (background and border become the same class)
    x[x == 0.5] = 1

    return x
from torch.utils.data import random_split

# Oxford IIIT Pets Segmentation dataset loaded via torchvision.
pets_path_train = os.path.join("final", 'OxfordPets', 'train')
pets_path_test = os.path.join("final", 'OxfordPets', 'test')
pets_train_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_train, split="trainval", target_types="segmentation", download=True,transform=transform,target_transform=target_transform)
pets_test_orig = torchvision.datasets.OxfordIIITPet(root=pets_path_test, split="test", target_types="segmentation", download=True,transform=transform,target_transform=target_transform)
print(type(pets_train_orig))
finetune_set, eval_set = random_split(pets_train_orig,[int(0.5*len(pets_train_orig))]*2)
finetune_dataloader = DataLoader(finetune_set, batch_size=32, shuffle=True)
eval_dataloader= DataLoader(eval_set, batch_size=32, shuffle=True)


num_epochs = 5
criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(segmentationModel.parameters(), lr=1e-4)

for epoch in range(num_epochs):
    segmentationModel.train()
    train_loss = 0.0
    for i, (images, masks) in enumerate(finetune_dataloader):
        images = images.to(device)
        masks = masks.to(device)
        outputs = segmentationModel(images)
        masks = label_segmentations(masks)
        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        train_loss += loss.item()
        print(f"[Epoch {epoch+1}/{num_epochs} Batch {i + 1}/{len(finetune_set)//32}]")

    train_loss /= len(finetune_dataloader)
    print(f"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}")

# Save the fine-tuned model
torch.save(segmentationModel.state_dict(), 'finetuned_model.pth')
import numpy as np
model.eval()
# testing
for epoch in range(1):
    segmentationModel.eval()
    ious=[]
    for i, (images, masks) in enumerate(eval_dataloader):
        images = images.to(device)
        masks = masks.to(device)

        outputs = segmentationModel(images)
        masks = label_segmentations(masks)
        inter = torch.logical_and(masks, outputs)
        uni = torch.logical_or(masks, outputs)

        ious.append(torch.count_nonzero(inter)/torch.count_nonzero(uni))

        loss = criterion(outputs, masks)
        loss.backward()
        optimizer.zero_grad()
        print(f"[Epoch {epoch+1}/{1} Batch {i + 1}/{len(eval_set)//32}]")


    print(f"Average intersction of union accuracy: {torch.mean(torch.as_tensor(ious))}")


# # Define hyperparameters
# pretrained_model_path = "pretrained_model.pth"
# num_classes = 2  # Number of segmentation classes (background and pet)
# batch_size = 32
# learning_rate = 1e-4
# num_epochs = 1

# from transformers import ViTImageProcessor
# # Load the dataset
# # image_processor = ViTImageProcessor.from_pretrained('facebook/vit-mae-base')

# def init_loader(data, size=20, work=8):
#     """
#     Initialises DataLoaders for training and testing purposes.
#     :param size: The desired batch size to use, by default this is set as 20.
#     :param work: The number of workers to use, by default this is set to be 8.
#     """
#     trainloader = torch.utils.data.DataLoader(
#         data.get_train(),
#         batch_size=size,
#         shuffle=True,
#         num_workers=work
#     )
#     testloader = torch.utils.data.DataLoader(
#         data.get_test(),
#         batch_size=size,
#         shuffle=True,
#         num_workers=work,
#     )
#     return trainloader, testloader




# data = OxfordData(    transform=transforms.Compose(
#     [
#         transforms.Resize((64, 64)),
#         transforms.ToTensor(),
#         # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
#     ]
# ))
# trainloader,testloader = init_loader(data)

# # Create the segmentation model
# model = SegmentationModel(model, num_classes)
# model.to(device)

# from torch import nn
# # Define loss function and optimizer
# criterion = nn.CrossEntropyLoss()
# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

# # Fine-tuning loop
# best_val_loss = float('inf')
# for epoch in range(num_epochs):
#     model.train()
#     train_loss = 0.0
#     for images, masks in trainloader:
#         images = images.to(device)
#         masks = masks.to(device)

#         optimizer.zero_grad()
#         outputs = model(images)
#         loss = criterion(outputs, masks)
#         loss.backward()
#         optimizer.step()

#         train_loss += loss.item() * images.size(0)

#     train_loss = train_loss / len(trainloader)

#     model.eval()
#     val_loss = 0.0
#     with torch.no_grad():
#         for images, masks in testloader:
#             images = images.to(device)
#             masks = masks.to(device)

#             outputs = model(images)
#             loss = criterion(outputs, masks)

#             val_loss += loss.item() * images.size(0)

#     val_loss = val_loss / len(testloader)

#     print(f"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f}")

#     if val_loss < best_val_loss:
#         best_val_loss = val_loss
#         torch.save(model.state_dict(), "best_model.pth")

# print("Fine-tuning completed. Best model saved as 'best_model.pth'.")

